
<!DOCTYPE html>

<html lang="en">
<head>
<meta charset="utf-8"/>
<meta content="width=device-width, initial-scale=1.0" name="viewport"/>
<title>Occlusion-Robust 2D and 3D Hand Pose Estimation for Rhesus Macaques</title>
<link href="styles.css" rel="stylesheet"/>
<style>
    body {
      background-color: #121212;
      color: #e0e0e0;
      font-family: 'Segoe UI', sans-serif;
      line-height: 1.6;
      margin: 0;
      padding: 0 1.5em;
    }
  
    h1, h2, h3, h4, h5 {
      color: #ffffff;
    }
  
    a {
      color: #90caf9;
    }
  
    a:hover {
      color: #bbdefb;
      text-decoration: underline;
    }
  
    nav ul {
      list-style: none;
      padding: 0;
      margin: 1em 0;
    }
  
    nav ul li {
      margin: 0.3em 0;
    }
  
    nav ul li a {
      text-decoration: none;
      font-weight: 600;
      color: #81d4fa;
    }
  
    nav ul li a:hover {
      text-decoration: underline;
    }
  
    pre, code {
      background-color: #1e1e1e;
      color: #cfcfcf;
      padding: 8px;
      border-radius: 4px;
      display: block;
      overflow-x: auto;
    }
  
    table {
      width: 100%;
      border-collapse: collapse;
      background-color: #1e1e1e;
      color: #e0e0e0;
    }
  
    th, td {
      border: 1px solid #333;
      padding: 10px;
      text-align: left;
    }
  
    th {
      background-color: #2c2c2c;
      color: #ffffff;
    }
  
    button {
      background-color: #2c7a7b;
      color: white;
      border: none;
      padding: 10px 16px;
      border-radius: 6px;
      cursor: pointer;
    }
  
    button:hover {
      background-color: #319795;
    }
  
    figcaption {
      color: #cccccc;
    }
  
    .subtitle {
      color: #aaa;
    }
  
    video {
      background-color: #000;
    }
  
    .gif-row img {
    width: 100%;
    max-width: 32%;
    height: auto;
    object-fit: contain;
    display: inline-block;
    }

    .gif-row {
    display: flex;
    justify-content: space-between;
    flex-wrap: wrap;
    margin-bottom: 16px;
    }

    .video-panel video {
    width: 100%;
    height: auto;
    max-height: 400px;
    object-fit: contain;
    border: 1px solid #444;
    border-radius: 6px;
    }

    .video-panel div {
    display: grid;
    grid-template-columns: repeat(auto-fit, minmax(250px, 1fr));
    gap: 8px;
    margin: 16px 0;
    }
  
  </style>
<script src="https://polyfill.io/v3/polyfill.min.js?features=es6"></script>
<script async="" id="MathJax-script" src="https://cdn.jsdelivr.net/npm/mathjax@3/es5/tex-mml-chtml.js">
</script>
</head>
<body>
<header>
<h1>Occlusion-Robust 2D and 3D Hand Pose Estimation for Rhesus Macaques</h1>
<p class="subtitle">An occlusion-aware pipeline combining multi-view capture and custom pose models</p>
<p class="author">
    Kanishka Mitra, MIT CSAIL — 
    <a href="mailto:mitra819@mit.edu">mitra819@mit.edu</a>
  </p>
<figure class="gif-row">
<div style="display: flex; justify-content: space-between;">
<img alt="Mediapipe Pose Tracking" src="./figs/mediapipe_pose_track.gif"/>
<img alt="PyBullet Reconstruction (Cam 3)" src="./figs/pb_recon_cam_3.gif"/>
<img alt="ResNet101 Overlay Predictions" src="./figs/train_cam_4_overlay_s13.gif"/>
</div>
<figcaption style="text-align: center; font-style: italic; margin-top: 0.5rem;""> Figure 1: From left to right – Mediapipe 2D pose tracking, 3D reconstruction in PyBullet, and overlay of ResNet101 predictions on camera 4.
</figcaption>

</figure>
</header>
    <nav>
    <ul>
    <li><a href="#background">Background</a></li>
    <li><a href="#methods">Methods</a></li>
    <li><a href="#training-rig">Training Rig</a></li>
    <li><a href="#calibration">Calibration</a></li>
    <li><a href="#models">Pose Models</a></li>
    <li><a href="#results">Results</a></li>
    <li><a href="#overlays">Calibration Overlays</a></li>
    <li><a href="#mpresults">Mediapipe Results</a></li>
    <li><a href="#quantitative">Quantitative Analysis</a></li>
    <li><a href="#conclusion">Conclusion</a></li>
    <li><a href="#references">References</a></li>
    </ul>
    </nav>
  <main>
    <section id='background'>
        <h2>Background & Prior Work</h2>
      
        <p>Dexterous manipulation is a hallmark of primate motor behavior, involving sophisticated planning, precise control, and rapid adaptation. Understanding these capabilities is essential not only from a neuroscience perspective but also for robotics, where imitation learning from biological agents offers the promise of more capable and flexible robotic systems. Despite substantial advancements in robot dexterity, current artificial systems remain limited in their ability to replicate the fluidity, adaptability, and efficiency of primate manipulation <a href="#ref1">[1]</a>, <a href="#ref2">[2]</a>, <a href="#ref3">[3]</a>. Monkeys, particularly rhesus macaques, exhibit remarkable manual dexterity, sometimes surpassing human abilities in specific manipulation tasks. Investigating these capabilities provides a unique opportunity to uncover novel motor strategies and neural computations that can significantly enhance robotic control frameworks <a href="#ref4">[4]</a>, <a href="#ref5">[5]</a>.</p>
      
        <p>Critically, the integration of neurophysiological recordings with detailed hand tracking data offers unprecedented opportunities to dissect the neural correlates of dexterous behavior. Implantable neural interfaces enable researchers to record neural signals directly from cortical motor regions while subjects engage in intricate manipulation tasks <a href="#ref6">[6]</a>, <a href="#ref7">[7]</a>, <a href="#ref8">[8]</a>. This combination holds the promise of illuminating the complete sensorimotor pathway—from sensory perception through motor planning to the execution of precise movements—potentially unlocking a holistic understanding of dexterous manipulation and informing biologically inspired artificial intelligence <a href="#ref9">[9]</a>, <a href="#ref10">[10]</a>.</p>
      
        <p>However, capturing accurate and detailed hand-pose data from rhesus macaques remains a significant technical challenge. Unlike humans, macaques exhibit rapid, unpredictable movements and interactions, coupled with substantial occlusions due to fur, tight inter-digit spacing, and frequent bimanual interactions. Existing marker-based approaches <a href="#ref11">[11]</a>, <a href="#ref12">[12]</a> are invasive, prone to marker loss, and unsuitable for naturalistic behavior. Current markerless tracking solutions developed primarily for human hands, such as Mediapipe <a href="#ref13">[13]</a> and OpenPose <a href="#ref14">[14]</a>, fail to generalize adequately to macaque anatomy and dynamic behavior, severely limiting their practical use. Furthermore, high-precision 3D methods typically require extensive hardware setups, making them infeasible for routine in-cage deployment.</p>
      
        <p>In the human domain, substantial progress has been made through large-scale datasets such as InterHand2.6M <a href="#ref15">[15]</a> and GigaHands <a href="#ref16">[16]</a>, which offer extensive benchmarks for hand interactions under controlled conditions. Architectures such as Transformers (HandsFormer <a href="#ref17">[17]</a>, OmniHands <a href="#ref18">[18]</a>) and probabilistic diffusion models (HandDiff <a href="#ref19">[19]</a>) have demonstrated robust performance for occluded hand pose estimation. However, these models rely heavily on human-centric priors and annotations, rendering them less effective for non-human primates. Multiview self-supervised methods, such as HaMuCo <a href="#ref20">[20]</a>, have shown promise by exploiting geometric consistency across multiple camera views. Yet, these approaches either involve computationally expensive training procedures or require densely arranged multi-camera setups impractical for standard primate lab environments. Recent work like OpenMonkeyStudio <a href="#ref21">[21]</a> has made strides toward markerless monkey tracking, but it is predominantly oriented towards whole-body tracking, lacking the necessary precision for detailed hand manipulation analysis.</p>
      
        <p>Addressing these significant gaps in the literature, our work introduces a practical and precise solution tailored specifically for tracking the 3D hand poses of rhesus macaques in realistic, partially occluded scenarios. We developed a streamlined, markerless pipeline built around a compact, synchronized six-camera rig, capturing high-resolution video data from multiple angles. To handle the unique anatomical and motion characteristics of macaque hands, we designed novel deep learning architectures incorporating anatomical priors explicitly informed by macaque biomechanics. Our proposed ResNeSt-50 model enhanced with bone-length priors ensures anatomical plausibility under occlusion and ambiguity, a critical innovation absent in previous methodologies.</p>
      
        <p>Through rigorous calibration, systematic validation against human-based benchmarks (Mediapipe), and detailed comparative analyses of multiple deep-learning architectures, our work not only addresses a fundamental technical barrier but also lays critical groundwork for future neuroscience and robotics research. By providing a reliable means to track macaque hand poses, we facilitate deeper exploration of the neural substrates of dexterous manipulation, ultimately enabling the development of bio-inspired robotic control systems that approach biological levels of sophistication and adaptability.</p>
      </section>
      

    <section id="methods">
      <h2>Methods</h2>
      <h3 id="training-rig">Training Rig</h3>
      <p>To accurately recover 3D keypoints under occlusion, multiple calibrated views are essential. A single camera cannot resolve depth ambiguities or occluded landmarks, particularly in bimanual interactions. Therefore, we designed and constructed a custom six-camera rig using synchronized FLIR Blackfly S cameras. The cameras were mounted in a semi-cylindrical arc configuration surrounding the subject at approximately eye level (Figure 2), enabling multi-angle coverage while minimizing blind spots caused by hand–hand occlusion.</p>

      <p>Each camera was hardware-synchronized to ensure sub-frame alignment, a critical requirement for stereo calibration and temporally coherent multi-view triangulation. Video was recorded at 164.9 FPS in full 1920×1200 resolution, allowing for crisp capture of fine finger articulations.</p>
      
      <p>We collected approximately two minutes of hand movement data from a human participant performing naturalistic finger and wrist motions (Figure 3). This dataset served as a controlled baseline to validate the full 2D-to-3D reconstruction pipeline before deploying models in more challenging real-world conditions.</p>
      
      <p>From these recordings, we extracted individual frames and labeled approximately 150 images using DeepLabCut. We annotated three anatomical landmarks per hand: the wrist joint (radiocarpal region), the metacarpophalangeal (MCP) joint of the index finger (second digit), and the MCP joint of the fifth digit (pinky). These points were selected due to their consistent spatial configuration and reliable visibility across frames. Furthermore, they enabled the computation of stable bone-length priors, as the relative distances between these landmarks tend to follow conserved biomechanical ratios. These priors proved critical for regularizing model predictions and ensuring anatomically plausible pose estimation, especially under occlusion.</p>
      
      <p>We then captured comparable recordings from a rhesus macaque interacting with foraging devices inside a primate training box. While we are unable to release video data from the macaque sessions due to restrictions set by MIT’s Division of Comparative Medicine (DCM), we present model outputs and blurred reconstructions derived from those recordings. These serve as a proof of concept that our system generalizes to non-human primates, even under complex occlusion and articulation dynamics.</p>
      
      <figure>
        <img 
          src="./figs/train_rig_lbl.png" 
          alt="Training Rig"
          style="max-width: 600px; width: 100%; height: auto; display: block; margin: 0 auto;"
        >
        <figcaption style="text-align: center; font-style: italic; margin-top: 0.5rem;">
          Figure 2: Our six-camera training rig setup for capturing human and macaque hand data.
        </figcaption>
      </figure>
      <figure class="video-panel">
        <div>
          <video loop autoplay muted><source src="./figs/train_cam_0.mp4" type="video/mp4"></video>
          <video loop autoplay muted><source src="./figs/train_cam_1.mp4" type="video/mp4"></video>
          <video loop autoplay muted><source src="./figs/train_cam_2.mp4" type="video/mp4"></video>
          <video loop autoplay muted><source src="./figs/train_cam_3.mp4" type="video/mp4"></video>
          <video loop autoplay muted><source src="./figs/train_cam_4.mp4" type="video/mp4"></video>
          <video loop autoplay muted><source src="./figs/train_cam_5.mp4" type="video/mp4"></video>
        </div>
        <figcaption style="text-align: center; font-style: italic; margin-top: 0.5rem;"> Figure 3: Recorded training videos from the six training rig cameras (camera 0 – camera 5).</figcaption>
      </figure>

      <h3 id="calibration">Camera Calibration and Triangulation</h3>
        <p>
        Accurate 3D hand pose estimation from multi-view video requires precise geometric calibration of the camera system. Specifically, it is essential to determine each camera's intrinsic and extrinsic parameters to enable reliable triangulation of 3D points from 2D detections. Here, we detail our systematic calibration procedure, including frame extraction, intrinsic calibration, extrinsic calibration, and subsequent triangulation, to robustly reconstruct occluded hand poses.
        </p>

        <p><strong>Frame Extraction and Dataset Preparation</strong><br>
        Calibration begins with the extraction of representative frames from synchronized video streams captured by each camera. To ensure uniform coverage and sufficient calibration accuracy, frames are extracted evenly across the entire video duration using:
        </p>

        <p>$$
        f_{\text{extract}} = \text{round}\left(\frac{T_{\text{video}}}{N_{\text{frames}}}\right)
        $$</p>

        <p>where \( T_{\text{video}} \) is the total number of video frames, and \( N_{\text{frames}} \) is the target number of calibration frames (typically 40–60 frames per camera). These frames contain images of a known calibration object, specifically a chessboard pattern with clearly defined corners, facilitating accurate feature detection for subsequent calibration.
        </p>

        <p><strong>Intrinsic Calibration</strong><br>
        For each camera independently, intrinsic calibration determines the internal camera parameters: the focal lengths \( (f_x, f_y) \), principal point \( (c_x, c_y) \), and radial and tangential distortion coefficients \( (k_1, k_2, p_1, p_2, k_3) \). We employed the Zhang calibration method implemented through OpenCV, optimizing intrinsic parameters by minimizing the reprojection error over detected chessboard corners. Specifically, we solve the following optimization:
        </p>

        <p>$$
        \underset{K,D}{\text{argmin}} \sum_{i=1}^{N} \left\| x_i - \text{project}(K, D, X_i, R_i, t_i) \right\|^2
        $$</p>

        <p>where:</p>

        <p>$$
        K = 
        \begin{bmatrix}
        f_x & 0 & c_x \\
        0 & f_y & c_y \\
        0 & 0 & 1
        \end{bmatrix},
        \quad
        D = [k_1, k_2, p_1, p_2, k_3]
        $$</p>

        <p>\( x_i \) is the observed 2D chessboard corner point,<br>
        \( X_i \) is the corresponding known 3D chessboard corner coordinate in the chessboard frame,<br>
        \( (R_i, t_i) \) represent the rotation and translation vectors from the world coordinate frame to the camera coordinate frame for each calibration image.
        </p>

        <p>This calibration step yields highly accurate estimates of the intrinsic parameters necessary for distortion correction and precise image formation modeling.
        </p>

        <p><strong>Extrinsic Calibration</strong><br>
        Following intrinsic calibration, extrinsic calibration is conducted pairwise between a reference camera and each other camera. This process computes the rotation matrix \( R \) and translation vector \( t \) describing each camera’s pose relative to a reference camera frame. Extrinsics were computed by jointly optimizing over pairs of matched chessboard corner detections across cameras, using the known intrinsic parameters to undistort images beforehand. Formally, this solves:
        </p>

        <p>$$
        \underset{R,t}{\text{argmin}} \sum_{j=1}^{M} \left\| x_j^{(\text{ref})} - \text{project}(K_{\text{ref}}, R, t, X_j) \right\|^2 + \left\| x_j^{(\text{other})} - \text{project}(K_{\text{other}}, I, 0, X_j) \right\|^2
        $$</p>

        <p>where:<br>
        \( x_j^{(\text{ref})}, x_j^{(\text{other})} \) are corresponding corner observations from the reference and other camera, respectively,<br>
        \( K_{\text{ref}}, K_{\text{other}} \) are intrinsic matrices for the reference and other cameras,<br>
        \( (R, t) \) represent the rotation and translation of the other camera relative to the reference camera.
        </p>

        <p>The optimization is conducted through the <code>cv2.stereoCalibrate</code> function with intrinsic parameters fixed, producing robust estimates of camera-to-camera geometry with minimal stereo reprojection error.
        </p>

        <p><strong>Triangulation of 3D Points</strong><br>
        Having obtained accurate intrinsic and extrinsic calibration parameters, we reconstruct 3D points from 2D observations via triangulation. Specifically, given at least two calibrated camera views, a 3D point \( X \) is reconstructed by solving:
        </p>

        <p>$$
        x_1 = P_1 X, \quad x_2 = P_2 X
        $$</p>

        <p>where \( P \) is the projection matrix, defined for each camera as:</p>

        <p>$$
        P = K [R \mid t]
        $$</p>

        <p>mapping a 3D world coordinate to its corresponding 2D image point.<br>
        Triangulation is performed using OpenCV's <code>cv2.triangulatePoints</code>, providing a solution via singular value decomposition (SVD). Formally, we have:
        </p>

        <p>$$
        X_h = \text{triangulate}(P_1, x_1, P_2, x_2), \quad X = \frac{X_h[0:3]}{X_h[3]}
        $$</p>

        <p>This yields a homogeneous 3D point \( X_h \), normalized to Euclidean space to retrieve precise 3D spatial positions.</p>

        <h3>Mediapipe as Ground Truth</h3>

        <p>We leveraged Google’s Mediapipe framework for human hand tracking as a baseline to validate our 3D reconstruction pipeline. Mediapipe offers real-time 2D keypoint detection using a palm detector followed by a regression model trained on large-scale human datasets. Its robustness under typical conditions makes it a valuable tool for debugging geometry and verifying system correctness.</p>
        
        <p>By applying Mediapipe to videos of human hand motion captured by our rig, we triangulated its 2D keypoints to obtain 3D trajectories. This served as a “sanity check” to confirm that our calibration and triangulation stages were functioning correctly. Mediapipe’s consistency across frames helped establish a reference pipeline before introducing more complex models.</p>
        
        <p>However, Mediapipe does not generalize to non-human primates. Differences in anatomy, fur, and motion patterns cause systematic tracking failures on macaque hands. While not usable for our final model, Mediapipe played a critical role in early-stage validation by providing clean, interpretable keypoints for controlled data.</p>
        
        <h3 id="models">Custom Pose Estimation Models</h3>

        <p>To robustly capture bimanual hand poses of rhesus macaques, we developed three custom deep learning architectures specifically adapted for precise localization of keypoints. Each model employs a ResNet-based backbone for effective feature extraction, while their heads differ to accommodate specific variations in structure and training objective. Here, we describe each model, emphasizing novel design choices and motivation.</p>

        <p><strong>Model 1: Baseline ResNet-101 Keypoint Estimator</strong><br>
        Our first model employs a ResNet-101 backbone, a standard but highly effective feature extractor commonly used in computer vision tasks due to its strong representational power. The ResNet architecture consists of sequential convolutional blocks enhanced with skip-connections, which facilitate stable training and mitigate gradient vanishing. This backbone processes input RGB images, extracting high-level feature representations.<br><br>
        For predicting keypoint locations, we append a heatmap-based head to the ResNet output. Specifically, we implement a Heatmap Gaussian Generator to construct heatmaps corresponding to each anatomical keypoint (e.g., wrists and finger joints), and we use a Heatmap Predictor that employs Weighted Mean Squared Error (MSE) loss for training:
        </p>

        <p>$$
        \mathcal{L}_{\text{heatmap}} = \frac{1}{N}\sum_{i=1}^{N}(H_i - \hat{H}_i)^2
        $$</p>

        <p>where \( H_i \) and \( \hat{H}_i \) denote predicted and ground-truth heatmaps respectively, averaged over \( N \) training examples.<br>
        This architecture provides a strong baseline for comparison with more specialized models.
        </p>

        <p><strong>Model 2: Enhanced ResNet-50 with Group Normalization</strong><br>
        Our second model is built upon a modified ResNet-50 backbone integrating Group Normalization (GN). Group Normalization partitions channels into groups and normalizes within each group, beneficial for smaller batch sizes typically encountered in high-resolution pose estimation tasks. This modification stabilizes training and improves robustness to variations in image contrast and lighting conditions.<br><br>
        The head structure is similar to Model 1, employing a heatmap-based prediction mechanism. However, we further integrate location refinement via a localization regression sub-head, optimizing a Weighted Huber loss to achieve precise sub-pixel localization:
        </p>

        <p>$$
        \mathcal{L}_{\text{locref}}(x) = 
        \begin{cases} 
        0.5(x - \hat{x})^2 & \text{if } |x - \hat{x}| < \delta, \\
        \delta(|x - \hat{x}| - 0.5\delta) & \text{otherwise}
        \end{cases}
        $$</p>

        <p>with \( \delta \) selected to balance sensitivity and robustness to outliers.</p>

        <p><strong>Model 3: Bone-Prior Enhanced ResNeSt-50</strong><br>
        Our third model, the most novel of our custom approaches, builds upon a ResNeSt-50 backbone, renowned for its superior representational capacity due to split-attention mechanisms. To address challenges associated with occlusion and joint ambiguity, we incorporate an anatomically-informed bone-length prior directly into the model’s loss function. This prior guides the network towards anatomically plausible predictions and significantly improves robustness to occlusions.
        </p>

        <p><strong>Formulation of the Bone-Prior Loss</strong><br>
        We define a set of bone connections \( \mathcal{B} \) between keypoints, each representing a pair of anatomically connected joints (e.g., wrist to finger joints). From labeled training data, we first compute the mean length \( \mu_b \) for each bone connection \( b \in \mathcal{B} \). The bone-prior loss component is thus defined as follows:
        </p>

        <p>$$
        \mathcal{L}_{\text{bone}} = \frac{1}{|\mathcal{B}|}\sum_{b \in \mathcal{B}} \left(\|J_{b1}-J_{b2}\|_2 - \mu_b\right)^2
        $$</p>

        <p>where \( J_{b1} \) and \( J_{b2} \) represent the predicted joint locations of bone endpoints. This loss term penalizes deviations from expected anatomical distances, effectively enforcing skeletal consistency.<br><br>
        The total training loss for this model becomes:
        </p>

        <p>$$
        \mathcal{L}_{\text{total}} = \lambda_{\text{heatmap}}\mathcal{L}_{\text{heatmap}} + \lambda_{\text{locref}}\mathcal{L}_{\text{locref}} + \lambda_{\text{bone}}\mathcal{L}_{\text{bone}}
        $$</p>

        <p>with hyperparameters \( \lambda_{\text{heatmap}}, \lambda_{\text{locref}}, \lambda_{\text{bone}} \) chosen empirically. By directly embedding anatomical priors into the training objective, this model significantly improves pose estimation robustness under occlusion.
        </p>

        <p><strong>Why Incorporate Bone-Length Priors?</strong><br>
        Incorporating bone-length priors addresses a critical weakness of standard heatmap-based models, particularly in challenging multi-hand scenarios. When keypoints become occluded, standard models typically rely on visual evidence alone, often resulting in anatomically impossible configurations. Our bone-length priors explicitly constrain predictions, leveraging anatomical knowledge to improve both accuracy and generalization, particularly when visual cues are limited or ambiguous.
        </p>

        <div style="text-align: center; margin: 24px 0;">
          <p><strong>Table 1:</strong> Comparison of the three custom pose estimation models.</p>
          <table border="1" cellpadding="8" cellspacing="0" style="margin: 0 auto; border-collapse: collapse;">
            <thead>
              <tr>
                <th>Attribute</th>
                <th>Model 1: Baseline ResNet-101</th>
                <th>Model 2: ResNet-50 + GN</th>
                <th>Model 3: ResNeSt-50 + Bone Prior</th>
              </tr>
            </thead>
            <tbody>
              <tr>
                <td>Backbone</td>
                <td>ResNet-101</td>
                <td>ResNet-50 w/ Group Norm</td>
                <td>ResNeSt-50 (Split-Attention)</td>
              </tr>
              <tr>
                <td>Normalization</td>
                <td>Batch Norm</td>
                <td>Group Norm</td>
                <td>Batch Norm</td>
              </tr>
              <tr>
                <td>Heatmap Prediction</td>
                <td>✓</td>
                <td>✓</td>
                <td>✓</td>
              </tr>
              <tr>
                <td>Localization Loss</td>
                <td>–</td>
                <td>Weighted Huber</td>
                <td>Weighted Huber</td>
              </tr>
              <tr>
                <td>Bone-Length Priors</td>
                <td>–</td>
                <td>–</td>
                <td>✓</td>
              </tr>
              <tr>
                <td>Key Innovation</td>
                <td>Strong baseline</td>
                <td>Improved normalization</td>
                <td>Anatomical priors for robustness</td>
              </tr>
            </tbody>
          </table>
        </div>        
    </section>
    <section id="results">
      <h2>Results</h2>
      <h3 id="overlays">Calibration Overlays</h3>
        <p>To validate the accuracy of our multi-view calibration, we visualized the projection of detected chessboard corners across all six FLIR Blackfly S cameras. The overlays (Figure 4) confirm precise reprojection alignment after both intrinsic and extrinsic calibration. Each camera’s intrinsic parameters—focal lengths, principal points, and distortion coefficients—were computed using OpenCV’s implementation of Zhang’s method. Extrinsics between camera pairs were optimized via stereo calibration using detected chessboard correspondences.</p>

        <p>We link our computed parameters below. These were critical to enabling stable 3D triangulation:</p>

        <figure class="video-panel">
            <div>
            <video loop autoplay muted><source src="./figs/cam_0_calib_overlay.mp4" type="video/mp4"></video>
            <video loop autoplay muted><source src="./figs/cam_1_calib_overlay.mp4" type="video/mp4"></video>
            <video loop autoplay muted><source src="./figs/cam_2_calib_overlay.mp4" type="video/mp4"></video>
            <video loop autoplay muted><source src="./figs/cam_3_calib_overlay.mp4" type="video/mp4"></video>
            <video loop autoplay muted><source src="./figs/cam_4_calib_overlay.mp4" type="video/mp4"></video>
            <video loop autoplay muted><source src="./figs/cam_5_calib_overlay.mp4" type="video/mp4"></video>
            </div>
            <figcaption style="text-align: center; font-style: italic; margin-top: 0.5rem;">Figure 4: Calibration overlay videos for cameras 0–5.</figcaption>
        </figure>

        <div class="button-panel">
            <a href="intrinsics.html" target="_blank"><button>Intrinsics</button></a>
            <a href="extrinsics.html" target="_blank"><button>Extrinsics</button></a>
        </div>

        <h3 id="mpresults">Mediapipe Results</h3>
        <p>To validate our triangulation pipeline, we used Google’s Mediapipe framework to extract 2D keypoints from synchronized video of a human subject (Figure 5). Mediapipe performed exceptionally well, yielding clean and temporally consistent keypoint detections. Using these predictions, we triangulated 3D joint trajectories via our calibrated projection matrices.</p>

        <p>The resulting reconstructions were remarkably stable and anatomically accurate, confirming the correctness of both our camera parameters and our triangulation implementation. Mediapipe thus served as a reliable source of ground-truth-like input during early system validation.</p>

        <figure class="video-panel">
            <div>
            <video loop autoplay muted><source src="./figs/mp_cam_0.mp4" type="video/mp4"></video>
            <video loop autoplay muted><source src="./figs/mp_cam_1.mp4" type="video/mp4"></video>
            <video loop autoplay muted><source src="./figs/mp_cam_2.mp4" type="video/mp4"></video>
            <video loop autoplay muted><source src="./figs/mp_cam_3.mp4" type="video/mp4"></video>
            <video loop autoplay muted><source src="./figs/mp_cam_4.mp4" type="video/mp4"></video>
            <video loop autoplay muted><source src="./figs/mp_cam_5.mp4" type="video/mp4"></video>
            </div>
            <figcaption style="text-align: center; font-style: italic; margin-top: 0.5rem;">Figure 5: Overlayed Mediapipe 2D hand‐pose tracking for cameras 0–5.</figcaption>
        </figure>

        <h4>3D Reconstruction Comparison</h4>
        <p>Using Mediapipe’s 2D outputs, we generated 3D hand reconstructions in PyBullet via triangulation. Interestingly, reconstructions using only a pair of calibrated cameras (Figure 6, left) yielded more stable 3D trajectories than those obtained using all six cameras (Figure 6, right).</p>

        <p>This counterintuitive result likely stems from small inter-camera inconsistencies in extrinsic calibration. When combining multiple baselines, even slight misalignments or timing jitter can compound, introducing reconstruction noise. Additionally, over-constraining the triangulation with redundant, imperfect baselines can cause instability—especially if some views contain partial occlusions or tracking errors.</p>

        <p><strong>To improve full-rig triangulation quality</strong>, we propose two potential solutions:</p>
        <ul>
          <li>
            <strong>Apply a robust, bundle-adjustment-style refinement across all views post-initial calibration.</strong><br>
            Bundle adjustment is a global optimization technique commonly used in structure-from-motion and multi-view geometry. It refines both camera parameters and 3D point estimates by minimizing the total reprojection error across all views. In our context, this would allow us to jointly refine the extrinsic calibration of each camera and the 3D keypoints, correcting small drift errors accumulated during pairwise stereo calibration. By considering all cameras simultaneously, this approach can resolve inconsistencies between overlapping views and significantly improve triangulation accuracy across the full rig.
          </li>
          <li>
            <strong>Incorporate confidence-weighted triangulation.</strong><br>
            Not all views contribute equally to accurate 3D reconstruction—some cameras may have occluded views, poor lighting, or calibration drift. By assigning confidence scores to each 2D detection (based on keypoint visibility or reprojection error), we can weight each view’s contribution during triangulation. This allows high-confidence views to dominate the reconstruction while down-weighting noisy or unreliable ones, improving the robustness of 3D estimates in the presence of occlusion or calibration artifacts.
          </li>
        </ul>


        <figure class="video-panel">
            <div style="display: grid; grid-template-columns: repeat(2, 1fr); gap: 8px; margin: 16px 0;">
            <video loop autoplay muted>
                <source src="./figs/pb_2_cam_calib_mp.mp4" type="video/mp4">
                Your browser doesn’t support HTML5 video.
            </video>
            <video loop autoplay muted>
                <source src="./figs/pb_all_cam_calib_mp.mp4" type="video/mp4">
                Your browser doesn’t support HTML5 video.
            </video>
            </div>
            <figcaption style="text-align: center; font-style: italic; margin-top: 0.5rem;">
            Figure 6: 3D hand‐pose reconstructions in PyBullet — two‐camera calibration (left) vs. all-camera calibration (right).
            </figcaption>
        </figure>
    <h3 id="quantitative">Quantitative Model Comparison</h3>
      <p>We first conducted a rigorous quantitative analysis of our three custom models: ResNet-101 (baseline), ResNet-50 with Group Normalization (GN), and ResNeSt-50 with an anatomically informed bone-length prior. Performance was assessed using standard metrics—mean Average Precision (mAP) and mean Average Recall (mAR)—computed over the final 10 epochs of training.</p>
      
      <figure>
        <img src="figs/final_plot.png" alt="Final Plot" style="width:48%; display:inline-block;">
        <img src="figs/resnet50_plot.png" alt="ResNet-50 Loss Over Time" style="width:48%; display:inline-block;">
        <figcaption style="text-align: center; font-style: italic; margin-top: 0.5rem;">Figure 7: Final train & val metrics (left) and ResNet-50 loss over time (right).</figcaption>
      </figure>
      
      <p>Figure 7, left highlights significant performance gaps. ResNet-101 substantially outperforms both ResNet-50 variants, achieving approximately 2.5 times higher mAP/mAR scores. This disparity emphasizes the critical role of network capacity: deeper models with broader receptive fields inherently capture finer-grained anatomical structures, necessary for high-precision hand pose estimation tasks.</p>
      
      <h4>Statistical Significance and Reliability</h4>
      
      <p>To assess if the bone-length prior offered genuine performance improvements, we performed paired t-tests comparing ResNet-50 baseline and bone-prior variants over the final epochs (Figure 7, right). All tests returned non-significant results (p > 0.05), indicating no measurable performance enhancement attributable to the bone-length prior under current hyperparameter settings. Notably, the average performance even slightly declined in the presence of the bone-length term, reinforcing that the prior, as implemented, was more restrictive than supportive.</p>

      <h4>Loss Landscape Analysis</h4>
      <figure>
        <img src="figs/loss_comp.png" alt="Loss Component Comparison" style="width:48%; display:inline-block;">
        <img src="figs/total_loss.png" alt="Total Loss Over Epochs" style="width:48%; display:inline-block;">
        <figcaption style="text-align: center; font-style: italic; margin-top: 0.5rem;">Figure 8: Loss-component proportions (left) and total loss trajectories (right).</figcaption>
      </figure>
      
      <p>To further diagnose differences between ResNet-50 models, we decomposed their total losses into constituent components (Figure 8). For the GN baseline, the loss comprised roughly 70% heatmap and 30% localization refinement, providing balanced gradients. However, with the introduction of the bone-length prior, the landscape drastically shifted: the bone-prior term rapidly dominated, constituting more than 95% of the total loss after approximately 20 epochs.</p>
      
      <p>This dominance reveals a phenomenon we refer to as "gradient starvation," where the critical heatmap and localization branches received increasingly negligible gradient signals. The stark initial spike and slow decay observed in the bone-prior run (Figure 8, left) illustrate a fundamentally different optimization landscape compared to the GN baseline, suggesting that excessive weighting of anatomical priors may inadvertently hinder the model’s ability to refine precise joint locations.</p>
      
      <h4>Stability and Generalization: The Advantage of Depth</h4>
      <figure>
        <img src="figs/rmse_plot.png" alt="RMSE Plot" style="width:60%; display:block; margin: 0 auto;">
        <figcaption style="text-align: center; font-style: italic; margin-top: 0.5rem;">Figure 9: ResNet-101 RMSE and PCK performance.</figcaption>
      </figure>
      
      <p>Figure 9 underscores the advantages conferred by a deeper network architecture. The ResNet-101 model rapidly converged to a low Root Mean Square Error (RMSE) (~47 px) by training step 170, with stability reflected in a consistent Percentage of Correct Keypoints (PCK) around 8%. In our context, RMSE measures the average pixel distance between the predicted keypoints and the labeled ground-truth keypoints in the image plane—lower values indicate higher spatial precision in keypoint localization. Meanwhile, PCK quantifies the proportion of keypoints predicted within a certain pixel threshold of the ground truth (e.g., within 10 pixels). A stable PCK indicates consistent prediction accuracy across frames. Together, these metrics provide a robust evaluation of both accuracy and reliability in pose estimation. The rapid and stable convergence observed in ResNet-101 suggests that deeper backbones offer superior feature representation and optimization stability, aided by frozen Batch Normalization layers pretrained on ImageNet, which reduce gradient noise in small-batch regimes.</p>

      <p>In contrast, the ResNet-50 variants displayed less stable optimization, particularly noticeable in total loss trajectories (Figure 8, right). Their flatter convergence curves and persistently higher total losses suggest underfitting, likely due to a combination of limited representational capacity and suboptimal training dynamics—namely, a constant learning rate without scheduled decays or restarts (e.g., cosine annealing), which can hinder convergence in shallower networks.</p>

      <h3>Qualitative Observations: Model Predictions</h3>

      <p>We further evaluated qualitative performance by inspecting video-based predictions from the two ResNet-50 variants. Both models demonstrated difficulties, producing visibly inaccurate joint predictions. The bone-prior model, despite enforcing plausible skeletal structures, struggled to accurately localize individual keypoints, likely due to gradient starvation from excessive prior weighting. The GN model predictions were marginally more accurate, though still coarse, suggesting the need for further optimization or richer architectures.</p>

      <p>To aid interpretation, each predicted keypoint is color-coded in the visualizations: left wrist (pink), left index MCP joint (blue), left pinky MCP joint (cyan), right wrist (green), right index MCP joint (yellow), and right pinky MCP joint (red). </p>
      <figure class="video-panel">
        <div style="display: grid; grid-template-columns: repeat(2, 1fr); gap: 8px;">
          <video loop autoplay muted controls>
            <source src="figs/overlay_baseline_iter_2.mp4" type="video/mp4">
            Your browser does not support HTML5 video.
          </video>
          <video loop autoplay muted controls>
            <source src="figs/overlay_boneprior_iter_2.mp4" type="video/mp4">
            Your browser does not support HTML5 video.
          </video>
        </div>
        <figcaption style="text-align: center; font-style: italic; margin-top: 0.5rem;">
          Figure 10: Side-by-side comparison of ResNet-50 GN (left) and Bone-Prior (right) model predictions.
        </figcaption>
      </figure>

      <p>Conversely, the ResNet-101 model excelled visually, generating precise and stable keypoint predictions for data from camera 4 (used in training). However, predictions degraded noticeably when evaluating on camera 1, an unseen viewpoint during training, revealing generalization limits inherent even in high-capacity models trained on limited views.</p>

      <figure class="video-panel">
        <div style="display: grid; grid-template-columns: repeat(2, 1fr); gap: 8px;">
          <video loop autoplay muted controls>
            <source src="figs/train_cam_4_overlay_s13.mp4" type="video/mp4">
            Your browser does not support HTML5 video.
          </video>
          <video loop autoplay muted controls>
            <source src="figs/train_cam_1_overlay_s13.mp4" type="video/mp4">
            Your browser does not support HTML5 video.
          </video>
        </div>
        <figcaption style="text-align: center; font-style: italic; margin-top: 0.5rem;">
          Figure 11: ResNet-101 predictions on training (left, camera 4) and novel (right, camera 1) viewpoints.
        </figcaption>
      </figure>

      <h4>Failure Modes and Recommendations</h4>
      <p>Our comprehensive analysis identifies clear failure modes for ResNet-50 variants:</p>
      <ul>
        <li><strong>Capacity limitation:</strong> The smaller backbones struggle to internalize detailed anatomical distributions critical for accurate macaque hand localization.</li>
        <li><strong>Gradient starvation:</strong> Excessive bone-prior weighting suppresses crucial heatmap gradients, halting fine-grained localization progress.</li>
        <li><strong>Slow learning rate decay:</strong> A constant Adam learning rate without cyclical restarts traps models in shallow minima, inhibiting deeper optimization.</li>
      </ul>
      
      <p>To mitigate these issues, we propose several practical adjustments:</p>
      <ul>
        <li><strong>Re-balance Bone-Prior Weight \( \lambda_{\text{bone}} \):</strong> Consider significantly lowering the bone-prior weight to approximately 10% of the heatmap loss, or gradually ramping it later in training, to maintain useful gradient flows.</li>
        <li><strong>Warm-start strategies:</strong> Initialize ResNet-50 variants using trained ResNet-101 weights for faster convergence and potential transfer of rich feature representations.</li>
        <li><strong>Adaptive Learning Rate Schedules:</strong> Employ cyclical LR methods with restarts (e.g., cosine-decay) to help models escape local minima, thereby improving both convergence speed and final accuracy.</li>
        <li><strong>Data-centric augmentation:</strong> Expand annotated training datasets significantly, and use aggressive offline augmentation (e.g., rotations &plusmn;40&deg;, scale variations) to enhance model robustness and generalization.</li>
        <li><strong>Per-Joint Evaluation:</strong> Track detailed metrics (e.g., per-joint PCK@10 px) to identify and address specific failure points, moving beyond aggregate metrics that obscure localized inaccuracies.</li>
      </ul>

      <h3><em>Put your monkey hands together</em></h3>

        <p>Up until this point, we have primarily focused on validating our methodology and pipeline using human datasets. These initial tests served as critical verification steps, establishing that our training procedures, evaluation metrics, and multi-camera reconstruction pipeline function robustly under controlled conditions. However, our core scientific question and ultimate goal revolve around the accurate tracking and 3D reconstruction of hand movements in rhesus macaques. The transition from controlled human data to more challenging non-human primate (NHP) data raises significant practical considerations, including differences in hand anatomy, motion dynamics, data quality, and ethical constraints regarding animal identification and privacy.</p>

        <p>To address these questions directly, we proceeded with a systematic pilot experiment: we collected approximately 5 minutes of free-moving monkey feeding and foraging data using our custom-designed 6-camera rig. From this footage, we extracted approximately 300 representative frames per camera, carefully annotated for two coarse but critical keypoints: the left hand and the right hand. Fine-grained finger-level annotations proved challenging due to image resolution constraints, animal movements, and labeling ambiguity. Therefore, our first goal was a reliable hand localization, as precise finger-tracking would require additional specialized methodologies.</p>

        <p>We leveraged our best-performing model, the ResNet101 architecture, to train a custom detector on these annotated frames. The training process was closely monitored, and the resultant learning curves are presented below (Figure 12).</p>

        <figure class="video-panel">
        <img src="figs/nhp_plot.png" alt="NHP loss and training curves" style="max-width: 100%; height: auto; margin: auto; display: block;">
        <figcaption style="text-align: center; font-style: italic; margin-top: 0.5rem;">Figure 12: Training and validation curves for NHP left and right hand tracking model (ResNet101).</figcaption>
        </figure>

        <p>The learning curves demonstrate a stable and rapid convergence, reaching saturation in both training and validation losses around epoch 40. Key performance metrics include a validation mAP of approximately 67.5% and an mAR of 78.2%. The RMSE plateaued around 5.3 pixels, indicating that the model is capable of localizing macaque hands with reasonable spatial accuracy relative to our image dimensions (1920 × 1200 pixels). Given the relatively small size of the dataset (1800 frames total), these results highlight impressive sample efficiency and underscore the feasibility of our approach.</p>

        <p>Despite this promising baseline, the clear performance gap between training (92.7% mAP) and validation (67.5% mAP) points towards notable generalization challenges. This generalization gap likely arises from camera-view imbalance (some viewpoints underrepresented), varied hand orientations, and frequent motion blur—common artifacts in unconstrained monkey footage.</p>

        <h4>Qualitative Results: 2D Tracking</h4>

        <p>To visually assess model performance and facilitate qualitative understanding, we generated videos where predictions of the left and right hand positions are overlayed onto the original frames. Adhering to ethical standards and DCM privacy guidelines, all videos have been thoroughly anonymized and blurred, rendering the monkey unidentifiable while clearly depicting our model's predictions.</p>

        <figure class="video-panel">
            <div style="display: grid; grid-template-columns: 2fr 1fr; gap: 12px;">
              <div style="display: grid; grid-template-rows: repeat(2, 1fr); gap: 8px;">
                <video loop autoplay muted controls>
                  <source src="figs/nhp_train_cam_1_overlay_soft.mp4" type="video/mp4">
                  Your browser does not support HTML5 video.
                </video>
                <video loop autoplay muted controls>
                  <source src="figs/nhp_train_cam_5_overlay_soft.mp4" type="video/mp4">
                  Your browser does not support HTML5 video.
                </video>
              </div>
              <img src="figs/3d_trace_nhp.png" alt="3D Trajectory Plot" style="width: 100%; height: auto; border-radius: 6px; object-fit: contain; box-shadow: 0 0 6px rgba(0,0,0,0.2);">
            </div>
            <figcaption style="text-align: center; font-style: italic; margin-top: 0.5rem;">
              Figure 13: Top left: Predictions on camera 1. Bottom left: Predictions on camera 5. Right: 3D hand trajectory plot from triangulated predictions.
            </figcaption>
          </figure>

        <p><strong>Camera 5 View:</strong> Overall, the model demonstrates solid performance in tracking the monkey’s hands. Predictions remain consistent across many frames, correctly identifying left and right hands with relatively high confidence (likelihood scores displayed prominently at the top corners). Nonetheless, occasional confusion between the left and right hands is apparent, particularly during moments of rapid hand movement or when hands closely overlap.</p>

        <p><strong>Camera 1 View:</strong> This viewpoint presents more challenges. Increased ambiguity due to overlapping hand movements frequently causes the model to mislabel left and right hands. Likelihood indicators fluctuate accordingly, illustrating clearly when the model becomes uncertain (low likelihood) or when predictions are confident (high likelihood). Despite these shortcomings, the predictions still provide valuable baseline tracking information, highlighting regions where targeted improvements are necessary.</p>

        <p><strong>3D Reconstruction:</strong> Having established intrinsic and extrinsic calibration parameters for our multi-camera rig using standard calibration procedures (as previously described), we next sought to reconstruct the hand positions in three-dimensional space from the 2D predictions. Using the ResNet101-derived 2D coordinates, we triangulated the data into a coherent 3D point cloud representation.</p>

        <p>Currently, the 3D reconstruction reveals substantial jitter and occasional mis-triangulations, notably exacerbated by momentary errors or low-confidence predictions from individual camera views. While clearly illustrating the potential of our approach, this initial reconstruction highlights the necessity of further refinement and filtering techniques—such as Kalman or particle filters, temporal consistency constraints, and robust statistical filtering—to yield more stable and scientifically meaningful 3D trajectories.</p>

        <h4>Lessons Learned and Next Steps</h4>

        <ul>
        <li><strong>Sample Efficiency and Model Capacity:</strong> Even with a modest dataset, ResNet101 achieves respectable results. However, performance ceilings indicate that increasing both dataset size and diversity (including viewpoint balance) is crucial.</li>
        <li><strong>Generalization and Ambiguity:</strong> Ambiguities introduced by overlapping hands, unusual viewpoints, and motion blur significantly limit accuracy. Strategic dataset augmentation (e.g., synthetic occlusion augmentation, blur augmentation, and viewpoint diversity) is highly recommended.</li>
        <li><strong>Confidence-based Tracking:</strong> Explicitly visualizing prediction confidence (likelihood) reveals clear patterns of model uncertainty, guiding targeted dataset enrichment.</li>
        <li><strong>3D Reconstruction Challenges:</strong> Raw triangulation from noisy 2D data leads to jittery and occasionally erroneous 3D results. Incorporating temporal smoothing methods (Kalman filtering, temporal convolutional networks, or Transformer-based approaches) will likely improve accuracy and stability.</li>
        <li><strong>Future Annotation Granularity:</strong> While initial results with hand-level annotation are promising, the eventual scientific aim necessitates finer-grained finger annotations. Future labeling efforts should aim to incrementally refine annotations down to individual digits, possibly leveraging automated pseudo-labeling strategies from high-performing human models to bootstrap accuracy.</li>
        </ul>


        <section id="conclusion">
            <h2>Conclusion</h2>
          
            <p>Our project presents an innovative pipeline for occlusion-robust, markerless 3D bimanual hand pose estimation specifically adapted for rhesus macaques. By integrating a custom-designed six-camera rig, rigorous geometric calibration, and novel deep learning models—particularly the ResNeSt-50 enhanced with an anatomically informed bone-length prior—we demonstrated substantial progress in tracking detailed hand articulations under significant occlusions. The unique application of bone-length priors represents a meaningful step forward in leveraging anatomical constraints within deep learning frameworks, offering a novel approach to improving pose robustness and accuracy.</p>
          
            <p>Despite these advancements, several key limitations emerged during our study. Most notably, smaller models like ResNet-50 faced challenges due to limited representational capacity and gradient starvation resulting from overly dominant bone-prior constraints. Additionally, inconsistencies in multi-camera calibration and extrinsic parameters introduced noise in 3D triangulations, particularly evident when integrating all camera views simultaneously. Furthermore, our trained models showed limited generalization capabilities when presented with viewpoints and conditions outside the original training set.</p>
          
            <p>Looking forward, our future work aims to refine the integration and optimization of anatomical priors, exploring adaptive weighting schemes and warm-start strategies to mitigate gradient starvation and improve convergence. We plan to enhance calibration stability through bundle-adjustment techniques and confidence-weighted triangulation. Expanding the dataset with diverse viewpoints and rigorous data augmentation strategies will further bolster model robustness and generalization. Ultimately, these developments promise substantial impacts across both computer vision—by setting new standards for pose estimation under occlusion—and neuroscience, enabling deeper insights into primate hand use, neural correlates of complex manipulation, and the foundations of motor control.</p>
          
            <p>In conclusion, our pilot effort successfully validates the feasibility of leveraging deep convolutional pose estimation models for NHP hand tracking and provides clear directions for future refinement. With expanded datasets, balanced viewpoints, targeted data augmentation, improved tracking algorithms, and more fine-grained annotations, we anticipate substantial performance gains. This will, in turn, facilitate more accurate and robust 3D hand reconstructions, supporting a wide range of neuroscientific and behavioral studies involving non-human primates.</p>
          </section>
          
      <section id="references">
        <h2>References</h2>
        <ol>
          <li id="ref1">Billard, A., & Kragic, D. (2019). Trends and Challenges in Robot Manipulation. <i>Science Robotics</i>.</li>
          <li id="ref2">Levine, S. et al. (2016). End-to-End Training of Deep Visuomotor Policies. <i>JMLR</i>.</li>
          <li id="ref3">Kumar, V. et al. (2020). Dexterous Robotic Manipulation with Deep Reinforcement Learning and Knowledge Transfer. <i>IEEE RA-L</i>.</li>
          <li id="ref4">Schaffelhofer, S., & Scherberger, H. (2016). Object vision to hand action in macaque parietal, premotor, and motor cortices. <i>eLife</i>.</li>
          <li id="ref5">Lemon, R. N. (2008). Descending pathways in motor control. <i>Annual Review of Neuroscience</i>.</li>
          <li id="ref6">Vyas, S. et al. (2020). Computation through neural population dynamics. <i>Annual Review of Neuroscience</i>.</li>
          <li id="ref7">Shenoy, K. V. et al. (2013). Cortical Control of Arm Movements: A Dynamical Systems Perspective. <i>Annual Review of Neuroscience</i>.</li>
          <li id="ref8">Churchland, M. M. et al. (2012). Neural population dynamics during reaching. <i>Nature</i>.</li>
          <li id="ref9">Richards, B. A. et al. (2019). A deep learning framework for neuroscience. <i>Nature Neuroscience</i>.</li>
          <li id="ref10">Jazayeri, M., & Afraz, A. (2017). Navigating the neural space in search of the neural code. <i>Neuron</i>.</li>
          <li id="ref11">Vargas-Irwin, C. E. et al. (2010). Decoding Complete Reach and Grasp Actions from Local Primary Motor Cortex Populations. <i>Journal of Neuroscience</i>.</li>
          <li id="ref12">Mason, C. R. et al. (2001). Manipulation by macaque monkeys: Simple and naturalistic tasks. <i>Behavioural Brain Research</i>.</li>
          <li id="ref13">Zhang, F. et al. (2020). Mediapipe hands: On-device real-time hand tracking. <i>CVPR Workshops</i>.</li>
          <li id="ref14">Cao, Z. et al. (2019). OpenPose: Realtime Multi-Person 2D Pose Estimation using Part Affinity Fields. <i>IEEE Transactions on Pattern Analysis and Machine Intelligence</i>.</li>
          <li id="ref15">Moon, G., Chang, J. Y., & Lee, K. M. (2020). InterHand2.6M: A dataset and baseline for 3D interacting hand pose estimation from a single RGB image. <i>European Conference on Computer Vision (ECCV)</i>.</li>
          <li id="ref16">Fu, Y., Zhang, X., Jiang, L., & Yuan, Y. (2024). GigaHands: One Billion Annotated Hand Poses for Vision-Based Dexterous Manipulation. <i>arXiv preprint arXiv:2412.04244</i>.</li>
          <li id="ref17">Hampali, S., Radwan, N., Zanfir, A., et al. (2022). Keypoint Transformer: Solving Joint Identification in Challenging Hand-Object Interactions. <i>CVPR</i>.</li>
          <li id="ref18">Lin, Z., Liu, X., Yu, Y., et al. (2024). OmniHands: Universal and Efficient Hand Representation Learning with Pretraining. <i>arXiv preprint arXiv:2405.20330</i>.</li>
          <li id="ref19">Cheng, Y., Zhou, T., Yang, W., et al. (2024). HandDiff: Diffusion-Based 3D Hand Pose Estimation from a Single RGB Image. <i>CVPR</i>.</li>
          <li id="ref20">Zheng, Y., Bao, W., Wang, X., et al. (2023). HaMuCo: Leveraging Multi-view Consistency for Semi-supervised Hand Mesh Recovery. <i>ICCV</i>.</li>
          <li id="ref21">Bala, P. C., Eisenreich, B. R., Yoo, S. B. M., et al. (2020). OpenMonkeyStudio: Automated markerless pose estimation in freely moving macaques. <i>bioRxiv</i>.</li>
        </ol>
      </section>
      <p style="font-size: 0.9em; color: #888; margin-top: 2em;">
        <em>Note:</em> This report was independently designed, implemented, and written by the author. AI tools (including ChatGPT and GitHub Copilot) were used to assist with formatting, debugging, and editing. All ideas, code, analysis, and implementations remain the original work of the author.
      </p>
      
      
    </main>
</body>
</html>
